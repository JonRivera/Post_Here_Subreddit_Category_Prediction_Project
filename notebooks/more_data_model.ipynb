{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johnrivera/.virtualenvs/ds_reddit_build-L6udXIR8/lib/python3.7/site-packages/pandas/compat/__init__.py:120: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import pickle\n",
    "\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../database/secrets\", \"r\") as file:\n",
    "    secrets = [i.strip('\\n') for i in file.readlines()]\n",
    "\n",
    "\n",
    "def conn_curs():\n",
    "    \"\"\"\n",
    "    makes a connection to the database dont worry these are dummy keys\n",
    "    \"\"\"\n",
    "\n",
    "    connection = psycopg2.connect(dbname=secrets[4], user=secrets[4],\n",
    "                                  password=secrets[5], host=secrets[6])\n",
    "    cursor = connection.cursor()\n",
    "    return connection, cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn, curs = conn_curs()\n",
    "df = pd.read_sql(\"SELECT * FROM posts\", conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Thousand Year Blood War Arc Anime Adaptation M...</td>\n",
       "      <td>bleach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Burn The Witch - Chapter 4 Discussion Thread #...</td>\n",
       "      <td>bleach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Let the journey begin.</td>\n",
       "      <td>bleach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Since Ichigos an English Literature major, thi...</td>\n",
       "      <td>bleach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>I just made these Ulquiorra customs for a clie...</td>\n",
       "      <td>bleach</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text subreddit\n",
       "0   1  Thousand Year Blood War Arc Anime Adaptation M...    bleach\n",
       "1   2  Burn The Witch - Chapter 4 Discussion Thread #...    bleach\n",
       "2   3                            Let the journey begin.     bleach\n",
       "3   4  Since Ichigos an English Literature major, thi...    bleach\n",
       "4   5  I just made these Ulquiorra customs for a clie...    bleach"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10782, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecidingToBeBetter    300\n",
       "legaladvice           300\n",
       "findapath             300\n",
       "WouldYouRather        300\n",
       "bleach                300\n",
       "getting_over_it       300\n",
       "immigration           300\n",
       "ShingekiNoKyojin      300\n",
       "MLPLounge             300\n",
       "Gundam                300\n",
       "LifeProTips           300\n",
       "Gunpla                300\n",
       "bettafish             300\n",
       "resumes               300\n",
       "leopardgeckos         300\n",
       "BackYardChickens      300\n",
       "GiftIdeas             300\n",
       "cats                  300\n",
       "deathnote             300\n",
       "nosurf                300\n",
       "araragi               300\n",
       "RBI                   300\n",
       "BokuNoHeroAcademia    300\n",
       "CaptainTsubasaDT      300\n",
       "ballpython            300\n",
       "anime                 300\n",
       "ferrets               300\n",
       "christmas             300\n",
       "Bedbugs               300\n",
       "DDLC                  300\n",
       "manga                 300\n",
       "ask                   300\n",
       "LegalAdviceUK         300\n",
       "selfhelp              300\n",
       "TooAfraidToAsk        300\n",
       "GetMotivated          220\n",
       "whatisthisthing        62\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.subreddit.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.text, df.subreddit, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pipe = make_pipeline(TfidfVectorizer(max_df=.95,  min_df=80), LogisticRegression(random_state=42, n_jobs=-1))\n",
    "log_pipe.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log train accuracy: 0.6422539111172643\n",
      "Log val accuracy: 0.46024164091036807\n"
     ]
    }
   ],
   "source": [
    "print(f\"Log train accuracy: {log_pipe.score(X_train, y_train)}\")\n",
    "print(f\"Log val accuracy: {log_pipe.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_pipe = make_pipeline(TfidfVectorizer(max_df=.95, min_df=80), LogisticRegression(random_state=42, n_jobs=-1))\n",
    "forest_pipe.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forest train accuracy: 0.6422539111172643\n",
      "Forest val accuracy: 0.46024164091036807\n"
     ]
    }
   ],
   "source": [
    "print(f\"Forest train accuracy: {forest_pipe.score(X_train, y_train)}\")\n",
    "print(f\"Forest val accuracy: {forest_pipe.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_pipe = make_pipeline(TfidfVectorizer(max_df=.95, min_df=80), SGDClassifier(n_jobs=-1))\n",
    "sgd_pipe.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD train accuracy: 0.7404125709538972\n",
      "SGD val accuracy: 0.4459117729699354\n"
     ]
    }
   ],
   "source": [
    "print(f\"SGD train accuracy: {sgd_pipe.score(X_train, y_train)}\")\n",
    "print(f\"SGD val accuracy: {sgd_pipe.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 150 candidates, totalling 450 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   25.1s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 450 out of 450 | elapsed:  5.6min finished\n"
     ]
    }
   ],
   "source": [
    "sgd_tune = make_pipeline(TfidfVectorizer(), SGDClassifier(n_jobs=-1))\n",
    "\n",
    "params = {\n",
    "    'sgdclassifier__loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "    'sgdclassifier__learning_rate': [\"constant\",\"optimal\",\"invscaling\",\"adaptive\"],\n",
    "    'sgdclassifier__eta0': [.001, .0001, .01],\n",
    "    'sgdclassifier__early_stopping': [True, False],\n",
    "    'sgdclassifier__validation_fraction': [.1, .2, .3],\n",
    "    'tfidfvectorizer__min_df': [30, 50, 80, 100, .1],\n",
    "    'tfidfvectorizer__max_df': [.95, .9, .97]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(sgd_tune, params, random_state=42, cv=3, n_jobs=-1, n_iter=150, verbose=1)\n",
    "\n",
    "search.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tfidfvectorizer__min_df': 30,\n",
       " 'tfidfvectorizer__max_df': 0.97,\n",
       " 'sgdclassifier__validation_fraction': 0.2,\n",
       " 'sgdclassifier__loss': 'squared_hinge',\n",
       " 'sgdclassifier__learning_rate': 'constant',\n",
       " 'sgdclassifier__eta0': 0.01,\n",
       " 'sgdclassifier__early_stopping': False}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5236060521154385"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'tfidfvectorizer__min_df': 30,\n",
    "#  'tfidfvectorizer__max_df': 0.97,\n",
    "#  'sgdclassifier__validation_fraction': 0.2,\n",
    "#  'sgdclassifier__loss': 'squared_hinge',\n",
    "#  'sgdclassifier__learning_rate': 'constant',\n",
    "#  'sgdclassifier__eta0': 0.01,\n",
    "#  'sgdclassifier__early_stopping': False}\n",
    "# 0.6216751552287852"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 150 candidates, totalling 450 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed: 11.0min\n",
      "[Parallel(n_jobs=-1)]: Done 450 out of 450 | elapsed: 11.5min finished\n"
     ]
    }
   ],
   "source": [
    "forest_tune = make_pipeline(TfidfVectorizer(), RandomForestClassifier(random_state=42, n_jobs=-1))\n",
    "\n",
    "params = {\n",
    "    'randomforestclassifier__n_estimators': range(100, 501, 100),\n",
    "    'randomforestclassifier__criterion': ['gini', 'entropy'],\n",
    "    'randomforestclassifier__max_depth': [None, 5, 10, 40, 100, 200],\n",
    "    'randomforestclassifier__min_samples_split': range(2, 51, 2),\n",
    "    'randomforestclassifier__min_samples_leaf': range(1, 51, 2),\n",
    "    'randomforestclassifier__max_features': ['auto', 'sqrt', 'log2'],\n",
    "    \n",
    "    'tfidfvectorizer__min_df': [30, 50, 80, 100, .1],\n",
    "    'tfidfvectorizer__max_df': [.95, .9, .97]\n",
    "}\n",
    "\n",
    "forest_search = RandomizedSearchCV(forest_tune, params, random_state=42, cv=3, n_jobs=-1, n_iter=150, verbose=1)\n",
    "\n",
    "forest_search.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tfidfvectorizer__min_df': 30,\n",
       " 'tfidfvectorizer__max_df': 0.97,\n",
       " 'randomforestclassifier__n_estimators': 100,\n",
       " 'randomforestclassifier__min_samples_split': 10,\n",
       " 'randomforestclassifier__min_samples_leaf': 3,\n",
       " 'randomforestclassifier__max_features': 'auto',\n",
       " 'randomforestclassifier__max_depth': 100,\n",
       " 'randomforestclassifier__criterion': 'gini'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47958054005459344"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MORE REDDITS NUKED OUR SCORES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRYING A NET BECAUSE SCORES ARE DEPRESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam, Adagrad, SGD, Ftrl, RMSprop\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(df.subreddit.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_df=0.95, min_df=80, stop_words='english')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = TfidfVectorizer(max_df=.95, min_df=80, stop_words='english')\n",
    "vect.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = os.path.join(\"logs\", \"Sigmoid + Dropout\")\n",
    "tensorboard_callback = TensorBoard(logdir, histogram_freq=1)\n",
    "stop = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, activation='sigmoid', input_dim=len(vect.get_feature_names())))\n",
    "model.add(Dropout(.1))\n",
    "model.add(Dense(128, activation='sigmoid'))\n",
    "model.add(Dropout(.1))\n",
    "model.add(Dense(128, activation='sigmoid'))\n",
    "model.add(Dropout(.1))\n",
    "model.add(Dense(df.subreddit.nunique(), activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=Adam(.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "  1/226 [..............................] - ETA: 0s - loss: 3.7475 - accuracy: 0.0000e+00WARNING:tensorflow:From /home/johnrivera/.virtualenvs/ds_reddit_build-L6udXIR8/lib/python3.7/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0020s vs `on_train_batch_end` time: 0.0097s). Check your callbacks.\n",
      "226/226 [==============================] - 1s 3ms/step - loss: 3.6321 - accuracy: 0.0295 - val_loss: 3.6170 - val_accuracy: 0.0216\n",
      "Epoch 2/200\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 3.6056 - accuracy: 0.0323 - val_loss: 3.5937 - val_accuracy: 0.0275\n",
      "Epoch 3/200\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 3.5410 - accuracy: 0.0602 - val_loss: 3.3934 - val_accuracy: 0.0840\n",
      "Epoch 4/200\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 3.2340 - accuracy: 0.0957 - val_loss: 3.0873 - val_accuracy: 0.1233\n",
      "Epoch 5/200\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 3.0157 - accuracy: 0.1470 - val_loss: 2.9142 - val_accuracy: 0.1773\n",
      "Epoch 6/200\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 2.8210 - accuracy: 0.1991 - val_loss: 2.7362 - val_accuracy: 0.2397\n",
      "Epoch 7/200\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 2.6335 - accuracy: 0.2550 - val_loss: 2.5712 - val_accuracy: 0.2787\n",
      "Epoch 8/200\n",
      "226/226 [==============================] - 0s 2ms/step - loss: 2.4701 - accuracy: 0.2934 - val_loss: 2.4334 - val_accuracy: 0.3119\n",
      "Epoch 9/200\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 2.3496 - accuracy: 0.3227 - val_loss: 2.3481 - val_accuracy: 0.3237\n",
      "Epoch 10/200\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 2.2537 - accuracy: 0.3438 - val_loss: 2.2827 - val_accuracy: 0.3470\n",
      "Epoch 11/200\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 2.1847 - accuracy: 0.3637 - val_loss: 2.2555 - val_accuracy: 0.3518\n",
      "Epoch 12/200\n",
      "226/226 [==============================] - 1s 5ms/step - loss: 2.1243 - accuracy: 0.3756 - val_loss: 2.2204 - val_accuracy: 0.3501\n",
      "Epoch 13/200\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 2.0794 - accuracy: 0.3897 - val_loss: 2.1949 - val_accuracy: 0.3706\n",
      "Epoch 14/200\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 2.0427 - accuracy: 0.3990 - val_loss: 2.1690 - val_accuracy: 0.3658\n",
      "Epoch 15/200\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 2.0115 - accuracy: 0.4032 - val_loss: 2.1746 - val_accuracy: 0.3709\n",
      "Epoch 16/200\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 1.9879 - accuracy: 0.4056 - val_loss: 2.1476 - val_accuracy: 0.3712\n",
      "Epoch 17/200\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 1.9670 - accuracy: 0.4141 - val_loss: 2.1356 - val_accuracy: 0.3762\n",
      "Epoch 18/200\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 1.9407 - accuracy: 0.4236 - val_loss: 2.1306 - val_accuracy: 0.3863\n",
      "Epoch 19/200\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 1.9220 - accuracy: 0.4278 - val_loss: 2.1217 - val_accuracy: 0.3852\n",
      "Epoch 20/200\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 1.9054 - accuracy: 0.4281 - val_loss: 2.1298 - val_accuracy: 0.3908\n",
      "Epoch 21/200\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 1.8822 - accuracy: 0.4371 - val_loss: 2.1362 - val_accuracy: 0.3908\n",
      "Epoch 22/200\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 1.8792 - accuracy: 0.4382 - val_loss: 2.1139 - val_accuracy: 0.3852\n",
      "Epoch 23/200\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 1.8510 - accuracy: 0.4465 - val_loss: 2.1285 - val_accuracy: 0.3889\n",
      "Epoch 24/200\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 1.8343 - accuracy: 0.4500 - val_loss: 2.1221 - val_accuracy: 0.3880\n",
      "Epoch 25/200\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 1.8333 - accuracy: 0.4552 - val_loss: 2.1107 - val_accuracy: 0.3942\n",
      "Epoch 26/200\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 1.8096 - accuracy: 0.4599 - val_loss: 2.1249 - val_accuracy: 0.3976\n",
      "Epoch 27/200\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 1.7910 - accuracy: 0.4601 - val_loss: 2.1294 - val_accuracy: 0.3925\n",
      "Epoch 28/200\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 1.7942 - accuracy: 0.4632 - val_loss: 2.1087 - val_accuracy: 0.3945\n",
      "Epoch 29/200\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 1.7821 - accuracy: 0.4660 - val_loss: 2.1199 - val_accuracy: 0.3936\n",
      "Epoch 30/200\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 1.7650 - accuracy: 0.4692 - val_loss: 2.1318 - val_accuracy: 0.3880\n",
      "Epoch 31/200\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 1.7570 - accuracy: 0.4736 - val_loss: 2.1206 - val_accuracy: 0.3993\n",
      "Epoch 32/200\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 1.7513 - accuracy: 0.4711 - val_loss: 2.1313 - val_accuracy: 0.3990\n",
      "Epoch 33/200\n",
      "226/226 [==============================] - 1s 4ms/step - loss: 1.7308 - accuracy: 0.4764 - val_loss: 2.1294 - val_accuracy: 0.4018\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5e32ee6490>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(np.array(vect.transform(X_train).todense()),le.transform(y_train),\n",
    "          validation_data=(np.array(vect.transform(X_test).todense()), le.transform(y_test)),\n",
    "          batch_size=32, epochs=200, callbacks=[tensorboard_callback, stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c67b4e90c2c8a51b\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c67b4e90c2c8a51b\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tring random search on net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(hidden_layers:int, largest_hidden_nueron:int, activation:str, out_activation, optimizer, learning_rate:float):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    # first layer is special so make out of for loop\n",
    "    x = [int(i) for i in np.linspace(64, largest_hidden_nueron, hidden_layers//2)]\n",
    "    y = x + list(reversed(x))\n",
    "    model.add(Dense(y[0], input_dim=len(vect.get_feature_names()), activation=activation))\n",
    "    if hidden_layers > 1:\n",
    "        for i in range(1, hidden_layers - 1):\n",
    "            model.add(Dense(y[i], activation=activation))\n",
    "    model.add(Dense(df.subreddit.nunique(), activation=out_activation))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer(learning_rate), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=create_model, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  7.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "452/452 [==============================] - 1s 2ms/step - loss: 3.6072 - accuracy: 0.0330 - val_loss: 3.5904 - val_accuracy: 0.0306\n",
      "Epoch 2/32\n",
      "452/452 [==============================] - 1s 2ms/step - loss: 3.5603 - accuracy: 0.0605 - val_loss: 3.5399 - val_accuracy: 0.0596\n",
      "Epoch 3/32\n",
      "452/452 [==============================] - 1s 2ms/step - loss: 3.4455 - accuracy: 0.1177 - val_loss: 3.3641 - val_accuracy: 0.1326\n",
      "Epoch 4/32\n",
      "452/452 [==============================] - 1s 2ms/step - loss: 3.2226 - accuracy: 0.1937 - val_loss: 3.1230 - val_accuracy: 0.2369\n",
      "Epoch 5/32\n",
      "452/452 [==============================] - 1s 2ms/step - loss: 2.9957 - accuracy: 0.2572 - val_loss: 2.9092 - val_accuracy: 0.2574\n",
      "Epoch 6/32\n",
      "452/452 [==============================] - 1s 2ms/step - loss: 2.7721 - accuracy: 0.2990 - val_loss: 2.7018 - val_accuracy: 0.2900\n",
      "Epoch 7/32\n",
      "452/452 [==============================] - 1s 2ms/step - loss: 2.5586 - accuracy: 0.3361 - val_loss: 2.5212 - val_accuracy: 0.3189\n",
      "Epoch 8/32\n",
      "452/452 [==============================] - 1s 2ms/step - loss: 2.3832 - accuracy: 0.3562 - val_loss: 2.3817 - val_accuracy: 0.3467\n",
      "Epoch 9/32\n",
      "452/452 [==============================] - 1s 2ms/step - loss: 2.2515 - accuracy: 0.3821 - val_loss: 2.2858 - val_accuracy: 0.3599\n",
      "Epoch 10/32\n",
      "452/452 [==============================] - 1s 2ms/step - loss: 2.1576 - accuracy: 0.3930 - val_loss: 2.2257 - val_accuracy: 0.3664\n",
      "Epoch 11/32\n",
      "452/452 [==============================] - 1s 2ms/step - loss: 2.0876 - accuracy: 0.4066 - val_loss: 2.1752 - val_accuracy: 0.3720\n",
      "Epoch 12/32\n",
      "452/452 [==============================] - 1s 2ms/step - loss: 2.0361 - accuracy: 0.4174 - val_loss: 2.1455 - val_accuracy: 0.3782\n",
      "Epoch 13/32\n",
      "452/452 [==============================] - 1s 2ms/step - loss: 1.9919 - accuracy: 0.4304 - val_loss: 2.1327 - val_accuracy: 0.3861\n",
      "Epoch 14/32\n",
      "452/452 [==============================] - 1s 2ms/step - loss: 1.9570 - accuracy: 0.4356 - val_loss: 2.1080 - val_accuracy: 0.3925\n",
      "Epoch 15/32\n",
      "452/452 [==============================] - 1s 2ms/step - loss: 1.9273 - accuracy: 0.4398 - val_loss: 2.1088 - val_accuracy: 0.3892\n",
      "Epoch 16/32\n",
      "452/452 [==============================] - 1s 2ms/step - loss: 1.9023 - accuracy: 0.4519 - val_loss: 2.0924 - val_accuracy: 0.4015\n",
      "Epoch 17/32\n",
      "452/452 [==============================] - 1s 2ms/step - loss: 1.8756 - accuracy: 0.4538 - val_loss: 2.0785 - val_accuracy: 0.4010\n",
      "Epoch 18/32\n",
      "452/452 [==============================] - 1s 2ms/step - loss: 1.8537 - accuracy: 0.4696 - val_loss: 2.0795 - val_accuracy: 0.3970\n",
      "Epoch 19/32\n",
      "452/452 [==============================] - 1s 2ms/step - loss: 1.8343 - accuracy: 0.4704 - val_loss: 2.0732 - val_accuracy: 0.3967\n",
      "Epoch 20/32\n",
      "452/452 [==============================] - 1s 2ms/step - loss: 1.8146 - accuracy: 0.4722 - val_loss: 2.0776 - val_accuracy: 0.3981\n",
      "Epoch 21/32\n",
      "452/452 [==============================] - 1s 2ms/step - loss: 1.7957 - accuracy: 0.4799 - val_loss: 2.0677 - val_accuracy: 0.4046\n",
      "Epoch 22/32\n",
      "452/452 [==============================] - 1s 2ms/step - loss: 1.7795 - accuracy: 0.4872 - val_loss: 2.0608 - val_accuracy: 0.4071\n",
      "Epoch 23/32\n",
      "452/452 [==============================] - 1s 2ms/step - loss: 1.7639 - accuracy: 0.4877 - val_loss: 2.0662 - val_accuracy: 0.4083\n",
      "Epoch 24/32\n",
      "452/452 [==============================] - 1s 2ms/step - loss: 1.7511 - accuracy: 0.4925 - val_loss: 2.0577 - val_accuracy: 0.4029\n",
      "Epoch 25/32\n",
      "452/452 [==============================] - 1s 2ms/step - loss: 1.7348 - accuracy: 0.4987 - val_loss: 2.0610 - val_accuracy: 0.4128\n",
      "Epoch 26/32\n",
      "452/452 [==============================] - 1s 2ms/step - loss: 1.7228 - accuracy: 0.5019 - val_loss: 2.0590 - val_accuracy: 0.4102\n",
      "Epoch 27/32\n",
      "452/452 [==============================] - 1s 2ms/step - loss: 1.7072 - accuracy: 0.5089 - val_loss: 2.0557 - val_accuracy: 0.4094\n",
      "Epoch 28/32\n",
      "452/452 [==============================] - 1s 2ms/step - loss: 1.6975 - accuracy: 0.5136 - val_loss: 2.0671 - val_accuracy: 0.4189\n",
      "Epoch 29/32\n",
      "452/452 [==============================] - 1s 2ms/step - loss: 1.6859 - accuracy: 0.5149 - val_loss: 2.0637 - val_accuracy: 0.4088\n",
      "Epoch 30/32\n",
      "452/452 [==============================] - 1s 2ms/step - loss: 1.6749 - accuracy: 0.5152 - val_loss: 2.0676 - val_accuracy: 0.4128\n",
      "Epoch 31/32\n",
      "452/452 [==============================] - 1s 2ms/step - loss: 1.6651 - accuracy: 0.5229 - val_loss: 2.0689 - val_accuracy: 0.4130\n",
      "Epoch 32/32\n",
      "452/452 [==============================] - 1s 2ms/step - loss: 1.6564 - accuracy: 0.5255 - val_loss: 2.0741 - val_accuracy: 0.4114\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=<tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x7f5e1c472dd0>,\n",
       "                   n_iter=20, n_jobs=-1,\n",
       "                   param_distributions={'activation': ('relu', 'sigmoid',\n",
       "                                                       'tanh'),\n",
       "                                        'batch_size': (16, 32, 64, 512),\n",
       "                                        'epochs': [100],\n",
       "                                        'hidden_layers': (1, 3, 5, 8),\n",
       "                                        'largest_hidden_nueron': [512, 256,\n",
       "                                                                  128],\n",
       "                                        'learning_rate': [0.001],\n",
       "                                        'optimizer': (<class...sorflow.python.keras.optimizer_v2.adam.Adam'>,\n",
       "                                                      <class 'tensorflow.python.keras.optimizer_v2.adagrad.Adagrad'>,\n",
       "                                                      <class 'tensorflow.python.keras.optimizer_v2.gradient_descent.SGD'>,\n",
       "                                                      <class 'tensorflow.python.keras.optimizer_v2.ftrl.Ftrl'>,\n",
       "                                                      <class 'tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop'>),\n",
       "                                        'out_activation': ['softmax']},\n",
       "                   random_state=42, verbose=1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'batch_size': (16,32,64,512),\n",
    "              'epochs': [100],\n",
    "              'hidden_layers': (1,3,5,8),\n",
    "              'largest_hidden_nueron': [512,256,128],\n",
    "              'activation': ('relu', 'sigmoid', 'tanh'),\n",
    "              'out_activation': ['softmax'],\n",
    "              'optimizer': (Adam, Adagrad, SGD, Ftrl, RMSprop),\n",
    "              'learning_rate': [.001]#tuple(np.linspace(.001, .01, 5))\n",
    "             }\n",
    "\n",
    "nueral_search = RandomizedSearchCV(estimator=model,param_distributions=param_grid,n_jobs=-1,cv=3,random_state=42,verbose=1,n_iter=20)\n",
    "nueral_search.fit(np.array(vect.transform(X_train).todense()),le.transform(y_train),\n",
    "                  validation_data=(np.array(vect.transform(X_test).todense()), le.transform(y_test)),\n",
    "                  epochs=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'out_activation': 'softmax',\n",
       " 'optimizer': tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop,\n",
       " 'learning_rate': 0.001,\n",
       " 'largest_hidden_nueron': 512,\n",
       " 'hidden_layers': 3,\n",
       " 'epochs': 100,\n",
       " 'batch_size': 16,\n",
       " 'activation': 'sigmoid'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nueral_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logdir = os.path.join(\"logs\", \"Best from search\")\n",
    "# tensorboard_callback = TensorBoard(logdir, histogram_freq=1)\n",
    "# stop = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10)\n",
    "\n",
    "# nn = Sequential([\n",
    "#     Dense(64, activation='sigmoid', input_dim=len(vect.get_feature_names())),\n",
    "#     Dense(128, activation='sigmoid'),\n",
    "#     Dense(128, activation='sigmoid'),\n",
    "#     Dense(64, activation='sigmoid')\n",
    "# ])\n",
    "# nn.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(.001), metrics=['accuracy'])\n",
    "\n",
    "# nn.fit(np.array(vect.transform(X_train).todense()),le.transform(y_train),\n",
    "#           validation_data=(np.array(vect.transform(X_test).todense()), le.transform(y_test)),\n",
    "#           batch_size=32, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THe best model for my data set ended up being the SDGC Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
