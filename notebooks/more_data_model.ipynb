{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import pickle\n",
    "\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../database/secrets\", \"r\") as file:\n",
    "    secrets = [i.strip('\\n') for i in file.readlines()]\n",
    "\n",
    "\n",
    "def conn_curs():\n",
    "    \"\"\"\n",
    "    makes a connection to the database dont worry these are dummy keys\n",
    "    \"\"\"\n",
    "\n",
    "    connection = psycopg2.connect(dbname=secrets[4], user=secrets[4],\n",
    "                                  password=secrets[5], host=secrets[6])\n",
    "    cursor = connection.cursor()\n",
    "    return connection, cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn, curs = conn_curs()\n",
    "df = pd.read_sql(\"SELECT * FROM posts\", conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Using This Subreddit # Rules\\n1. [Be excellent...</td>\n",
       "      <td>MovieSuggestions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Looking for a movie where a mother/father go i...</td>\n",
       "      <td>MovieSuggestions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>The Abyss  \\n\\nHi Everyone. I had originally p...</td>\n",
       "      <td>MovieSuggestions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Good plot twist Hi all,\\n\\nAny suggestions for...</td>\n",
       "      <td>MovieSuggestions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>featuring characters who have interesting reas...</td>\n",
       "      <td>MovieSuggestions</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text         subreddit\n",
       "0   1  Using This Subreddit # Rules\\n1. [Be excellent...  MovieSuggestions\n",
       "1   2  Looking for a movie where a mother/father go i...  MovieSuggestions\n",
       "2   3  The Abyss  \\n\\nHi Everyone. I had originally p...  MovieSuggestions\n",
       "3   4  Good plot twist Hi all,\\n\\nAny suggestions for...  MovieSuggestions\n",
       "4   5  featuring characters who have interesting reas...  MovieSuggestions"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15935, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AskWomen                300\n",
       "learnpython             300\n",
       "RedditWritesSeinfeld    300\n",
       "Jokes                   300\n",
       "teenagers               300\n",
       "britishproblems         300\n",
       "ADHD                    300\n",
       "DecidingToBeBetter      300\n",
       "socialskills            300\n",
       "soccer                  300\n",
       "dating_advice           300\n",
       "philosophy              300\n",
       "TalesFromRetail         300\n",
       "offmychest              300\n",
       "nfl                     300\n",
       "gaming                  300\n",
       "Showerthoughts          300\n",
       "ShouldIbuythisgame      300\n",
       "hockey                  300\n",
       "AskMen                  300\n",
       "CasualConversation      300\n",
       "3amjokes                300\n",
       "politics                300\n",
       "explainlikeimfive       300\n",
       "bestoflegaladvice       300\n",
       "MovieSuggestions        300\n",
       "AmItheAsshole           300\n",
       "askscience              300\n",
       "socialanxiety           300\n",
       "talesfromtechsupport    300\n",
       "tifu                    300\n",
       "whowouldwin             300\n",
       "HailCorporate           300\n",
       "WritingPrompts          300\n",
       "pcmasterrace            300\n",
       "PoliticalDiscussion     300\n",
       "patientgamers           300\n",
       "dadjokes                300\n",
       "Games                   300\n",
       "linux                   300\n",
       "TellMeAFact             300\n",
       "seduction               300\n",
       "boringdystopia          300\n",
       "KarmaCourt              300\n",
       "bestof                  300\n",
       "worldnews               300\n",
       "books                   300\n",
       "LifeProTips             300\n",
       "nosleep                 300\n",
       "godtiersuperpowers      300\n",
       "learnSQL                300\n",
       "television              300\n",
       "OutOfTheLoop            185\n",
       "ProRevenge              129\n",
       "newreddits               21\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.subreddit.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.text, df.subreddit, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pipe = make_pipeline(TfidfVectorizer(max_df=.95,  min_df=80), LogisticRegression(random_state=42, n_jobs=-1))\n",
    "log_pipe.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log train accuracy: 0.7729486699138254\n",
      "Log val accuracy: 0.608290549534132\n"
     ]
    }
   ],
   "source": [
    "print(f\"Log train accuracy: {log_pipe.score(X_train, y_train)}\")\n",
    "print(f\"Log val accuracy: {log_pipe.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidfvectorizer', TfidfVectorizer(max_df=0.95, min_df=80)),\n",
       "                ('logisticregression',\n",
       "                 LogisticRegression(n_jobs=-1, random_state=42))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_pipe = make_pipeline(TfidfVectorizer(max_df=.95, min_df=80), LogisticRegression(random_state=42, n_jobs=-1))\n",
    "forest_pipe.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forest train accuracy: 0.7729486699138254\n",
      "Forest val accuracy: 0.608290549534132\n"
     ]
    }
   ],
   "source": [
    "print(f\"Forest train accuracy: {forest_pipe.score(X_train, y_train)}\")\n",
    "print(f\"Forest val accuracy: {forest_pipe.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_pipe = make_pipeline(TfidfVectorizer(max_df=.95, min_df=80), SGDClassifier(n_jobs=-1))\n",
    "sgd_pipe.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD train accuracy: 0.8768265267890596\n",
      "SGD val accuracy: 0.6035367940673132\n"
     ]
    }
   ],
   "source": [
    "print(f\"SGD train accuracy: {sgd_pipe.score(X_train, y_train)}\")\n",
    "print(f\"SGD val accuracy: {sgd_pipe.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 150 candidates, totalling 450 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   56.8s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 450 out of 450 | elapsed:  2.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=Pipeline(steps=[('tfidfvectorizer',\n",
       "                                              TfidfVectorizer()),\n",
       "                                             ('sgdclassifier',\n",
       "                                              SGDClassifier(n_jobs=-1))]),\n",
       "                   n_iter=150, n_jobs=-1,\n",
       "                   param_distributions={'sgdclassifier__early_stopping': [True,\n",
       "                                                                          False],\n",
       "                                        'sgdclassifier__eta0': [0.001, 0.0001,\n",
       "                                                                0.01],\n",
       "                                        'sgdclassifier__learning_rate': ['constant',\n",
       "                                                                         'optimal',\n",
       "                                                                         'invscaling',\n",
       "                                                                         'adaptive'],\n",
       "                                        'sgdclassifier__loss': ['hinge', 'log',\n",
       "                                                                'modified_huber',\n",
       "                                                                'squared_hinge',\n",
       "                                                                'perceptron'],\n",
       "                                        'sgdclassifier__validation_fraction': [0.1,\n",
       "                                                                               0.2,\n",
       "                                                                               0.3],\n",
       "                                        'tfidfvectorizer__max_df': [0.95, 0.9,\n",
       "                                                                    0.97],\n",
       "                                        'tfidfvectorizer__min_df': [30, 50, 80,\n",
       "                                                                    100, 0.1]},\n",
       "                   random_state=42, verbose=1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_tune = make_pipeline(TfidfVectorizer(), SGDClassifier(n_jobs=-1))\n",
    "\n",
    "params = {\n",
    "    'sgdclassifier__loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "    'sgdclassifier__learning_rate': [\"constant\",\"optimal\",\"invscaling\",\"adaptive\"],\n",
    "    'sgdclassifier__eta0': [.001, .0001, .01],\n",
    "    'sgdclassifier__early_stopping': [True, False],\n",
    "    'sgdclassifier__validation_fraction': [.1, .2, .3],\n",
    "    'tfidfvectorizer__min_df': [30, 50, 80, 100, .1],\n",
    "    'tfidfvectorizer__max_df': [.95, .9, .97]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(sgd_tune, params, random_state=42, cv=3, n_jobs=-1, n_iter=150, verbose=1)\n",
    "\n",
    "search.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tfidfvectorizer__min_df': 30,\n",
       " 'tfidfvectorizer__max_df': 0.97,\n",
       " 'sgdclassifier__validation_fraction': 0.2,\n",
       " 'sgdclassifier__loss': 'squared_hinge',\n",
       " 'sgdclassifier__learning_rate': 'constant',\n",
       " 'sgdclassifier__eta0': 0.01,\n",
       " 'sgdclassifier__early_stopping': False}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6216751552287852"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'tfidfvectorizer__min_df': 30,\n",
    "#  'tfidfvectorizer__max_df': 0.97,\n",
    "#  'sgdclassifier__validation_fraction': 0.2,\n",
    "#  'sgdclassifier__loss': 'squared_hinge',\n",
    "#  'sgdclassifier__learning_rate': 'constant',\n",
    "#  'sgdclassifier__eta0': 0.01,\n",
    "#  'sgdclassifier__early_stopping': False}\n",
    "# 0.6216751552287852"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 150 candidates, totalling 450 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   13.6s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 450 out of 450 | elapsed:  4.2min finished\n"
     ]
    }
   ],
   "source": [
    "forest_tune = make_pipeline(TfidfVectorizer(), RandomForestClassifier(random_state=42, n_jobs=-1))\n",
    "\n",
    "params = {\n",
    "    'randomforestclassifier__n_estimators': range(100, 501, 100),\n",
    "    'randomforestclassifier__criterion': ['gini', 'entropy'],\n",
    "    'randomforestclassifier__max_depth': [None, 5, 10, 40, 100, 200],\n",
    "    'randomforestclassifier__min_samples_split': range(2, 51, 2),\n",
    "    'randomforestclassifier__min_samples_leaf': range(1, 51, 2),\n",
    "    'randomforestclassifier__max_features': ['auto', 'sqrt', 'log2'],\n",
    "    \n",
    "    'tfidfvectorizer__min_df': [30, 50, 80, 100, .1],\n",
    "    'tfidfvectorizer__max_df': [.95, .9, .97]\n",
    "}\n",
    "\n",
    "forest_search = RandomizedSearchCV(forest_tune, params, random_state=42, cv=3, n_jobs=-1, n_iter=150, verbose=1)\n",
    "\n",
    "forest_search.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tfidfvectorizer__min_df': 30,\n",
       " 'tfidfvectorizer__max_df': 0.97,\n",
       " 'randomforestclassifier__n_estimators': 100,\n",
       " 'randomforestclassifier__min_samples_split': 10,\n",
       " 'randomforestclassifier__min_samples_leaf': 3,\n",
       " 'randomforestclassifier__max_features': 'auto',\n",
       " 'randomforestclassifier__max_depth': 100,\n",
       " 'randomforestclassifier__criterion': 'gini'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5651931942195753"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MORE REDDITS NUKED OUR SCORES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRYING A NET BECAUSE SCORES ARE DEPRESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam, Adagrad, SGD, Ftrl, RMSprop\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(df.subreddit.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_df=0.95, min_df=80, stop_words='english')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = TfidfVectorizer(max_df=.95, min_df=80, stop_words='english')\n",
    "vect.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = os.path.join(\"logs\", \"Sigmoid + Dropout\")\n",
    "tensorboard_callback = TensorBoard(logdir, histogram_freq=1)\n",
    "stop = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=5)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, activation='sigmoid', input_dim=len(vect.get_feature_names())))\n",
    "model.add(Dropout(.1))\n",
    "model.add(Dense(128, activation='sigmoid'))\n",
    "model.add(Dropout(.1))\n",
    "model.add(Dense(128, activation='sigmoid'))\n",
    "model.add(Dropout(.1))\n",
    "model.add(Dense(df.subreddit.nunique(), activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=Adam(.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "  1/334 [..............................] - ETA: 0s - loss: 4.3227 - accuracy: 0.0312WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0018s vs `on_train_batch_end` time: 0.0241s). Check your callbacks.\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 4.0212 - accuracy: 0.0203 - val_loss: 3.9953 - val_accuracy: 0.0394\n",
      "Epoch 2/200\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 3.9814 - accuracy: 0.0232 - val_loss: 3.9158 - val_accuracy: 0.0346\n",
      "Epoch 3/200\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 3.5993 - accuracy: 0.0756 - val_loss: 3.3291 - val_accuracy: 0.1451\n",
      "Epoch 4/200\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 3.1982 - accuracy: 0.1471 - val_loss: 2.9804 - val_accuracy: 0.2151\n",
      "Epoch 5/200\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 2.8438 - accuracy: 0.2250 - val_loss: 2.6634 - val_accuracy: 0.2696\n",
      "Epoch 6/200\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 2.5500 - accuracy: 0.2877 - val_loss: 2.4392 - val_accuracy: 0.3223\n",
      "Epoch 7/200\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 2.3643 - accuracy: 0.3264 - val_loss: 2.3109 - val_accuracy: 0.3558\n",
      "Epoch 8/200\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 2.2274 - accuracy: 0.3684 - val_loss: 2.2140 - val_accuracy: 0.3913\n",
      "Epoch 9/200\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 2.1054 - accuracy: 0.4009 - val_loss: 2.1305 - val_accuracy: 0.4060\n",
      "Epoch 10/200\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 2.0045 - accuracy: 0.4331 - val_loss: 2.0795 - val_accuracy: 0.4237\n",
      "Epoch 11/200\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 1.9370 - accuracy: 0.4568 - val_loss: 2.0295 - val_accuracy: 0.4402\n",
      "Epoch 12/200\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 1.8563 - accuracy: 0.4718 - val_loss: 2.0162 - val_accuracy: 0.4432\n",
      "Epoch 13/200\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 1.8168 - accuracy: 0.4843 - val_loss: 1.9808 - val_accuracy: 0.4533\n",
      "Epoch 14/200\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 1.7614 - accuracy: 0.5004 - val_loss: 1.9645 - val_accuracy: 0.4592\n",
      "Epoch 15/200\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 1.7189 - accuracy: 0.5067 - val_loss: 1.9693 - val_accuracy: 0.4602\n",
      "Epoch 16/200\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 1.6696 - accuracy: 0.5207 - val_loss: 1.9440 - val_accuracy: 0.4640\n",
      "Epoch 17/200\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 1.6338 - accuracy: 0.5303 - val_loss: 1.9306 - val_accuracy: 0.4729\n",
      "Epoch 18/200\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 1.5992 - accuracy: 0.5461 - val_loss: 1.9217 - val_accuracy: 0.4748\n",
      "Epoch 19/200\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 1.5764 - accuracy: 0.5517 - val_loss: 1.9346 - val_accuracy: 0.4777\n",
      "Epoch 20/200\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 1.5418 - accuracy: 0.5591 - val_loss: 1.9068 - val_accuracy: 0.4856\n",
      "Epoch 21/200\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 1.5209 - accuracy: 0.5596 - val_loss: 1.9048 - val_accuracy: 0.4845\n",
      "Epoch 22/200\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 1.4982 - accuracy: 0.5666 - val_loss: 1.9029 - val_accuracy: 0.4862\n",
      "Epoch 23/200\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 1.4747 - accuracy: 0.5777 - val_loss: 1.9032 - val_accuracy: 0.4870\n",
      "Epoch 24/200\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 1.4442 - accuracy: 0.5809 - val_loss: 1.8936 - val_accuracy: 0.4955\n",
      "Epoch 25/200\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 1.4252 - accuracy: 0.5879 - val_loss: 1.8922 - val_accuracy: 0.4957\n",
      "Epoch 26/200\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 1.3921 - accuracy: 0.5975 - val_loss: 1.8909 - val_accuracy: 0.4965\n",
      "Epoch 27/200\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 1.3774 - accuracy: 0.6006 - val_loss: 1.9005 - val_accuracy: 0.5016\n",
      "Epoch 28/200\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 1.3531 - accuracy: 0.6082 - val_loss: 1.9029 - val_accuracy: 0.5018\n",
      "Epoch 29/200\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 1.3394 - accuracy: 0.6083 - val_loss: 1.9092 - val_accuracy: 0.5020\n",
      "Epoch 30/200\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 1.3163 - accuracy: 0.6173 - val_loss: 1.9059 - val_accuracy: 0.5054\n",
      "Epoch 31/200\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 1.3012 - accuracy: 0.6226 - val_loss: 1.9096 - val_accuracy: 0.5068\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5e70076a20>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(np.array(vect.transform(X_train).todense()),le.transform(y_train),\n",
    "          validation_data=(np.array(vect.transform(X_test).todense()), le.transform(y_test)),\n",
    "          batch_size=32, epochs=200, callbacks=[tensorboard_callback, stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-7120cc43fbea27d8\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-7120cc43fbea27d8\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tring random search on net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(hidden_layers:int, largest_hidden_nueron:int, activation:str, out_activation, optimizer, learning_rate:float):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    # first layer is special so make out of for loop\n",
    "    x = [int(i) for i in np.linspace(64, largest_hidden_nueron, hidden_layers//2)]\n",
    "    y = x + list(reversed(x))\n",
    "    model.add(Dense(y[0], input_dim=len(vect.get_feature_names()), activation=activation))\n",
    "    if hidden_layers > 1:\n",
    "        for i in range(1, hidden_layers - 1):\n",
    "            model.add(Dense(y[i], activation=activation))\n",
    "    model.add(Dense(df.subreddit.nunique(), activation=out_activation))\n",
    "    # Compile model\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer(learning_rate), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=create_model, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:   11.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 4.0059 - accuracy: 0.0170 - val_loss: 3.9980 - val_accuracy: 0.0194\n",
      "Epoch 2/32\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 3.9950 - accuracy: 0.0168 - val_loss: 3.9882 - val_accuracy: 0.0323\n",
      "Epoch 3/32\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 3.8215 - accuracy: 0.0383 - val_loss: 3.5549 - val_accuracy: 0.0540\n",
      "Epoch 4/32\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 3.4418 - accuracy: 0.0758 - val_loss: 3.3371 - val_accuracy: 0.1017\n",
      "Epoch 5/32\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 3.1461 - accuracy: 0.1337 - val_loss: 3.0360 - val_accuracy: 0.1483\n",
      "Epoch 6/32\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 2.8900 - accuracy: 0.1796 - val_loss: 2.8682 - val_accuracy: 0.1698\n",
      "Epoch 7/32\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 2.7330 - accuracy: 0.2050 - val_loss: 2.7948 - val_accuracy: 0.1825\n",
      "Epoch 8/32\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 2.6335 - accuracy: 0.2346 - val_loss: 2.7249 - val_accuracy: 0.2111\n",
      "Epoch 9/32\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 2.5613 - accuracy: 0.2516 - val_loss: 2.6822 - val_accuracy: 0.2270\n",
      "Epoch 10/32\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 2.4955 - accuracy: 0.2709 - val_loss: 2.6741 - val_accuracy: 0.2265\n",
      "Epoch 11/32\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 2.4501 - accuracy: 0.2838 - val_loss: 2.6226 - val_accuracy: 0.2379\n",
      "Epoch 12/32\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 2.4000 - accuracy: 0.2920 - val_loss: 2.5891 - val_accuracy: 0.2613\n",
      "Epoch 13/32\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 2.3502 - accuracy: 0.3092 - val_loss: 2.5964 - val_accuracy: 0.2558\n",
      "Epoch 14/32\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 2.2991 - accuracy: 0.3247 - val_loss: 2.5326 - val_accuracy: 0.2786\n",
      "Epoch 15/32\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 2.2526 - accuracy: 0.3399 - val_loss: 2.4942 - val_accuracy: 0.2957\n",
      "Epoch 16/32\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 2.2048 - accuracy: 0.3526 - val_loss: 2.4785 - val_accuracy: 0.3018\n",
      "Epoch 17/32\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 2.1620 - accuracy: 0.3655 - val_loss: 2.4382 - val_accuracy: 0.3162\n",
      "Epoch 18/32\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 2.1204 - accuracy: 0.3770 - val_loss: 2.4118 - val_accuracy: 0.3290\n",
      "Epoch 19/32\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 2.0867 - accuracy: 0.3961 - val_loss: 2.4178 - val_accuracy: 0.3290\n",
      "Epoch 20/32\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 2.0534 - accuracy: 0.4030 - val_loss: 2.3862 - val_accuracy: 0.3451\n",
      "Epoch 21/32\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 2.0272 - accuracy: 0.4152 - val_loss: 2.3804 - val_accuracy: 0.3442\n",
      "Epoch 22/32\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 1.9998 - accuracy: 0.4241 - val_loss: 2.3506 - val_accuracy: 0.3535\n",
      "Epoch 23/32\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 1.9737 - accuracy: 0.4307 - val_loss: 2.3474 - val_accuracy: 0.3523\n",
      "Epoch 24/32\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 1.9520 - accuracy: 0.4322 - val_loss: 2.3327 - val_accuracy: 0.3603\n",
      "Epoch 25/32\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 1.9316 - accuracy: 0.4445 - val_loss: 2.3425 - val_accuracy: 0.3613\n",
      "Epoch 26/32\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 1.9151 - accuracy: 0.4475 - val_loss: 2.3578 - val_accuracy: 0.3575\n",
      "Epoch 27/32\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 1.8957 - accuracy: 0.4544 - val_loss: 2.3352 - val_accuracy: 0.3689\n",
      "Epoch 28/32\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 1.8836 - accuracy: 0.4577 - val_loss: 2.3523 - val_accuracy: 0.3683\n",
      "Epoch 29/32\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 1.8665 - accuracy: 0.4624 - val_loss: 2.3161 - val_accuracy: 0.3700\n",
      "Epoch 30/32\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 1.8526 - accuracy: 0.4607 - val_loss: 2.3322 - val_accuracy: 0.3740\n",
      "Epoch 31/32\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 1.8387 - accuracy: 0.4718 - val_loss: 2.3416 - val_accuracy: 0.3706\n",
      "Epoch 32/32\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 1.8195 - accuracy: 0.4788 - val_loss: 2.3164 - val_accuracy: 0.3809\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=<tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x7f5efc361eb8>,\n",
       "                   n_iter=20, n_jobs=-1,\n",
       "                   param_distributions={'activation': ('relu', 'sigmoid',\n",
       "                                                       'tanh'),\n",
       "                                        'batch_size': (16, 32, 64, 512),\n",
       "                                        'epochs': [100],\n",
       "                                        'hidden_layers': (1, 3, 5, 8),\n",
       "                                        'largest_hidden_nueron': [512, 256,\n",
       "                                                                  128],\n",
       "                                        'learning_rate': [0.001],\n",
       "                                        'optimizer': (<class...sorflow.python.keras.optimizer_v2.adam.Adam'>,\n",
       "                                                      <class 'tensorflow.python.keras.optimizer_v2.adagrad.Adagrad'>,\n",
       "                                                      <class 'tensorflow.python.keras.optimizer_v2.gradient_descent.SGD'>,\n",
       "                                                      <class 'tensorflow.python.keras.optimizer_v2.ftrl.Ftrl'>,\n",
       "                                                      <class 'tensorflow.python.keras.optimizer_v2.rmsprop.RMSprop'>),\n",
       "                                        'out_activation': ['softmax']},\n",
       "                   random_state=42, verbose=1)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'batch_size': (16,32,64,512),\n",
    "              'epochs': [100],\n",
    "              'hidden_layers': (1,3,5,8),\n",
    "              'largest_hidden_nueron': [512,256,128],\n",
    "              'activation': ('relu', 'sigmoid', 'tanh'),\n",
    "              'out_activation': ['softmax'],\n",
    "              'optimizer': (Adam, Adagrad, SGD, Ftrl, RMSprop),\n",
    "              'learning_rate': [.001]#tuple(np.linspace(.001, .01, 5))\n",
    "             }\n",
    "\n",
    "nueral_search = RandomizedSearchCV(estimator=model,param_distributions=param_grid,n_jobs=-1,cv=3,random_state=42,verbose=1,n_iter=20)\n",
    "nueral_search.fit(np.array(vect.transform(X_train).todense()),le.transform(y_train),\n",
    "                  validation_data=(np.array(vect.transform(X_test).todense()), le.transform(y_test)),\n",
    "                  epochs=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'out_activation': 'softmax',\n",
       " 'optimizer': tensorflow.python.keras.optimizer_v2.adam.Adam,\n",
       " 'learning_rate': 0.001,\n",
       " 'largest_hidden_nueron': 128,\n",
       " 'hidden_layers': 5,\n",
       " 'epochs': 100,\n",
       " 'batch_size': 32,\n",
       " 'activation': 'sigmoid'}"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nueral_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logdir = os.path.join(\"logs\", \"Best from search\")\n",
    "# tensorboard_callback = TensorBoard(logdir, histogram_freq=1)\n",
    "# stop = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10)\n",
    "\n",
    "# nn = Sequential([\n",
    "#     Dense(64, activation='sigmoid', input_dim=len(vect.get_feature_names())),\n",
    "#     Dense(128, activation='sigmoid'),\n",
    "#     Dense(128, activation='sigmoid'),\n",
    "#     Dense(64, activation='sigmoid')\n",
    "# ])\n",
    "# nn.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(.001), metrics=['accuracy'])\n",
    "\n",
    "# nn.fit(np.array(vect.transform(X_train).todense()),le.transform(y_train),\n",
    "#           validation_data=(np.array(vect.transform(X_test).todense()), le.transform(y_test)),\n",
    "#           batch_size=32, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
