{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.layers import LSTM, SpatialDropout1D, Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from numpy import array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from PostGres SQL Data Base\n",
    "with open(\"../database/secrets\", \"r\") as file:\n",
    "    secrets = [i.strip('\\n') for i in file.readlines()]\n",
    "\n",
    "\n",
    "def conn_curs():\n",
    "    \"\"\"\n",
    "    makes a connection to the database dont worry these are dummy keys\n",
    "    \"\"\"\n",
    "\n",
    "    connection = psycopg2.connect(dbname=secrets[4], user=secrets[4],\n",
    "                                  password=secrets[5], host=secrets[6])\n",
    "    cursor = connection.cursor()\n",
    "    return connection, cursor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn, curs = conn_curs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql(\"SELECT * FROM posts\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Thousand Year Blood War Arc Anime Adaptation M...</td>\n",
       "      <td>bleach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Burn The Witch - Chapter 4 Discussion Thread #...</td>\n",
       "      <td>bleach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Let the journey begin.</td>\n",
       "      <td>bleach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Since Ichigos an English Literature major, thi...</td>\n",
       "      <td>bleach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>I just made these Ulquiorra customs for a clie...</td>\n",
       "      <td>bleach</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text subreddit\n",
       "0   1  Thousand Year Blood War Arc Anime Adaptation M...    bleach\n",
       "1   2  Burn The Witch - Chapter 4 Discussion Thread #...    bleach\n",
       "2   3                            Let the journey begin.     bleach\n",
       "3   4  Since Ichigos an English Literature major, thi...    bleach\n",
       "4   5  I just made these Ulquiorra customs for a clie...    bleach"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/johnrivera/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#imports to clean text\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import regular expressions to filter/clean appropiate text\n",
    "import re\n",
    "df = df.reset_index(drop=True)\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "    text = text.replace('x', '')\n",
    "#    text = re.sub(r'\\W+', '', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
    "    return text\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "df['text'] = df['text'].str.replace('\\d+', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>thousand year blood war arc anime adaptation m...</td>\n",
       "      <td>bleach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>burn witch chapter discussion thread #chapter ...</td>\n",
       "      <td>bleach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>let journey begin</td>\n",
       "      <td>bleach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>since ichigos english literature major would s...</td>\n",
       "      <td>bleach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>made ulquiorra customs client mine huge bleach...</td>\n",
       "      <td>bleach</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text subreddit\n",
       "0   1  thousand year blood war arc anime adaptation m...    bleach\n",
       "1   2  burn witch chapter discussion thread #chapter ...    bleach\n",
       "2   3                                  let journey begin    bleach\n",
       "3   4  since ichigos english literature major would s...    bleach\n",
       "4   5  made ulquiorra customs client mine huge bleach...    bleach"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 75901 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 50000\n",
    "# Max number of words in each post`\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(df['text'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (29788, 250)\n"
     ]
    }
   ],
   "source": [
    "X = tokenizer.texts_to_sequences(df['text'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (29788, 100)\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(df['subreddit']).values\n",
    "print('Shape of label tensor:', Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bleach', 'BackYardChickens', 'resumes', 'ballpython',\n",
       "       'TooAfraidToAsk', 'Gunpla', 'christmas', 'cats', 'DDLC',\n",
       "       'immigration', 'MLPLounge', 'bettafish', 'Bedbugs', 'araragi',\n",
       "       'getting_over_it', 'findapath', 'parrots', 'dbz',\n",
       "       'Dragonballsuper', 'Gundam', 'selfhelp', 'snakes',\n",
       "       'whatsthisplant', 'asklaw', 'Petloss', 'Berserk', 'KissAnime',\n",
       "       'RATS', 'reptiles', 'HunterXHunter', 'Gifts', 'DarlingInTheFranxx',\n",
       "       'LegalAdviceUK', 'ferrets', 'RBI', 'LifeProTips', 'Rabbits',\n",
       "       'GiftIdeas', 'ShingekiNoKyojin', 'ask', 'leopardgeckos',\n",
       "       'CaptainTsubasaDT', 'BokuNoHeroAcademia', 'anime',\n",
       "       'DecidingToBeBetter', 'whatsthisbug', 'manga', 'legaladvice',\n",
       "       'nosurf', 'shrimptank', 'whatisthisthing', 'deathnote',\n",
       "       'WouldYouRather', 'GetMotivated', 'AusLegal', 'whatstheword',\n",
       "       'CasualConversation', 'ImmigrationCanada', 'getdisciplined',\n",
       "       'Beekeeping', 'BeardedDragons', 'ifyoulikeblank', 'declutter',\n",
       "       'NoStupidQuestions', 'hamsters', 'pestcontrol', 'help', 'IWantOut',\n",
       "       'legaladviceofftopic', 'needadvice', 'NameThatSong', 'tarantulas',\n",
       "       'answers', 'legal', 'translator', 'productivity', 'fatestaynight',\n",
       "       'StardustCrusaders', 'makemychoice', 'Animesuggest', 'Pets',\n",
       "       'selfimprovement', 'explainlikeimfive', 'INeedAName', 'guineapigs',\n",
       "       'HelpMeFind', 'whatsthatbook', 'dogs', 'Clairvoyantreadings',\n",
       "       'changemyview', 'whatsthisworth', 'theydidthemath', 'ukvisa',\n",
       "       'puppy101', 'AquaSwap', 'StopGaming', 'tipofmytongue',\n",
       "       'datarecovery', 'CatAdvice', 'Dogtraining'], dtype=object)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26809, 250) (26809, 100)\n",
      "(2979, 250) (2979, 100)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "377/377 [==============================] - 116s 308ms/step - loss: 4.2170 - accuracy: 0.0356 - val_loss: 3.9250 - val_accuracy: 0.0630\n",
      "Epoch 2/5\n",
      "377/377 [==============================] - 134s 354ms/step - loss: 3.6801 - accuracy: 0.0952 - val_loss: 3.4829 - val_accuracy: 0.1167\n",
      "Epoch 3/5\n",
      "377/377 [==============================] - 162s 431ms/step - loss: 3.1688 - accuracy: 0.1737 - val_loss: 3.1308 - val_accuracy: 0.2085\n",
      "Epoch 4/5\n",
      "377/377 [==============================] - 181s 479ms/step - loss: 2.6983 - accuracy: 0.2781 - val_loss: 2.8216 - val_accuracy: 0.2663\n",
      "Epoch 5/5\n",
      "377/377 [==============================] - 148s 391ms/step - loss: 2.2495 - accuracy: 0.3811 - val_loss: 2.5918 - val_accuracy: 0.3204\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(100, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "377/377 [==============================] - 156s 414ms/step - loss: 4.2238 - accuracy: 0.0351 - val_loss: 4.0090 - val_accuracy: 0.0705\n",
      "Epoch 2/30\n",
      "377/377 [==============================] - 164s 434ms/step - loss: 3.6316 - accuracy: 0.1024 - val_loss: 3.3951 - val_accuracy: 0.1466\n",
      "Epoch 3/30\n",
      "377/377 [==============================] - 138s 367ms/step - loss: 3.0164 - accuracy: 0.2031 - val_loss: 2.9231 - val_accuracy: 0.2372\n",
      "Epoch 4/30\n",
      "377/377 [==============================] - 149s 395ms/step - loss: 2.4987 - accuracy: 0.3119 - val_loss: 2.6348 - val_accuracy: 0.3100\n",
      "Epoch 5/30\n",
      "377/377 [==============================] - 128s 339ms/step - loss: 2.1154 - accuracy: 0.4081 - val_loss: 2.5117 - val_accuracy: 0.3480\n",
      "Epoch 6/30\n",
      "377/377 [==============================] - 122s 323ms/step - loss: 1.7724 - accuracy: 0.5036 - val_loss: 2.3014 - val_accuracy: 0.4088\n",
      "Epoch 7/30\n",
      "377/377 [==============================] - 121s 321ms/step - loss: 1.4904 - accuracy: 0.5770 - val_loss: 2.2930 - val_accuracy: 0.4226\n",
      "Epoch 8/30\n",
      "377/377 [==============================] - 130s 344ms/step - loss: 1.2545 - accuracy: 0.6450 - val_loss: 2.2318 - val_accuracy: 0.4495\n",
      "Epoch 9/30\n",
      "377/377 [==============================] - 142s 378ms/step - loss: 1.0682 - accuracy: 0.7000 - val_loss: 2.2518 - val_accuracy: 0.4565\n",
      "Epoch 10/30\n",
      "377/377 [==============================] - 128s 339ms/step - loss: 0.9047 - accuracy: 0.7477 - val_loss: 2.2255 - val_accuracy: 0.4715\n",
      "Epoch 11/30\n",
      "377/377 [==============================] - 131s 348ms/step - loss: 0.7644 - accuracy: 0.7846 - val_loss: 2.3042 - val_accuracy: 0.4767\n",
      "Epoch 12/30\n",
      "377/377 [==============================] - 133s 352ms/step - loss: 0.6827 - accuracy: 0.8092 - val_loss: 2.3294 - val_accuracy: 0.4715\n",
      "Epoch 13/30\n",
      "377/377 [==============================] - 130s 344ms/step - loss: 0.5703 - accuracy: 0.8426 - val_loss: 2.3879 - val_accuracy: 0.4849\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(100, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 64\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deathnote\n"
     ]
    }
   ],
   "source": [
    "# Testing W/ Bleach Post\n",
    "post = [\"\"\"In episode 119 after Ikkaku used Bankai on Edrad it talked about\n",
    "Ikkaku's backstory and he trained Renji when he didn't know how to fight. \n",
    "I looked for it in the manga but they skipped that scene.\n",
    "I really want to know because they performed a mountain level feat\"\"\"]\n",
    "seq = tokenizer.texts_to_sequences(post)\n",
    "padded = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "pred = model.predict(padded)\n",
    "labels = df['subreddit'].unique()\n",
    "print(labels[np.argmax(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
